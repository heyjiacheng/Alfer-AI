RED PANDA BACKEND API TEST LOG
Generated on: 2025-03-26 21:20:21
================================================================================


================================================================================
TIMESTAMP: 2025-03-26 21:20:21
CURL COMMAND:
curl -X GET http://localhost:8080/knowledge-bases

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "knowledge_bases": [
    {
      "created_at": "2025-03-26 19:27:08",
      "description": "\u7cfb\u7edf\u9ed8\u8ba4\u77e5\u8bc6\u5e93",
      "id": 57,
      "name": "\u9ed8\u8ba4\u77e5\u8bc6\u5e93"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:21
CURL COMMAND:
curl -X POST http://localhost:8080/knowledge-bases -H "Content-Type: application/json" -d '{"name": "Test Knowledge Base", "description": "Testing the API functionality"}'

RESPONSE STATUS: 201
RESPONSE BODY:
{
  "knowledge_base_id": 59,
  "message": "success with knowledge"
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:21
CURL COMMAND:
curl -X GET http://localhost:8080/knowledge-bases

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "knowledge_bases": [
    {
      "created_at": "2025-03-26 20:20:21",
      "description": "Testing the API functionality",
      "id": 59,
      "name": "Test Knowledge Base"
    },
    {
      "created_at": "2025-03-26 19:27:08",
      "description": "\u7cfb\u7edf\u9ed8\u8ba4\u77e5\u8bc6\u5e93",
      "id": 57,
      "name": "\u9ed8\u8ba4\u77e5\u8bc6\u5e93"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:21
CURL COMMAND:
curl -X GET http://localhost:8080/knowledge-bases/59

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "created_at": "2025-03-26 20:20:21",
  "description": "Testing the API functionality",
  "documents": [],
  "id": 59,
  "name": "Test Knowledge Base"
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:21
CURL COMMAND:
curl -X PUT http://localhost:8080/knowledge-bases/59 -H "Content-Type: application/json" -d '{"name": "Updated Test Knowledge Base", "description": "Updated description for testing"}'

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "message": "knowledge base updated"
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:28
CURL COMMAND:
curl -X POST http://localhost:8080/upload/59 -F file=@/Users/jiadengxu/Documents/rekep.pdf

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "document_id": 46,
  "message": "\u6587\u6863\u5d4c\u5165\u6210\u529f"
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:31
CURL COMMAND:
curl -X GET http://localhost:8080/documents

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "documents": [
    {
      "file_size": 13823419,
      "id": 46,
      "knowledge_base_id": 59,
      "original_filename": "rekep.pdf",
      "upload_date": "2025-03-26 20:20:28"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:31
CURL COMMAND:
curl -X GET http://localhost:8080/documents

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "documents": [
    {
      "file_size": 13823419,
      "id": 46,
      "knowledge_base_id": 59,
      "original_filename": "rekep.pdf",
      "upload_date": "2025-03-26 20:20:28"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:31
CURL COMMAND:
curl -X GET http://localhost:8080/documents/46

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "extraction_failed": 0,
  "file_path": "./documents/1743020421.734411_rekep.pdf",
  "file_size": 13823419,
  "id": 46,
  "knowledge_base_id": 59,
  "metadata": null,
  "original_filename": "rekep.pdf",
  "stored_filename": "1743020421.734411_rekep.pdf",
  "upload_date": "2025-03-26 20:20:28"
}


================================================================================
TIMESTAMP: 2025-03-26 21:20:31
CURL COMMAND:
curl -X POST http://localhost:8080/conversations -H "Content-Type: application/json" -d '{"title": "Test Conversation", "knowledge_base_id": 59}'

RESPONSE STATUS: 201
RESPONSE BODY:
{
  "conversation_id": 16,
  "message": "conversation created"
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:02
CURL COMMAND:
curl -X POST http://localhost:8080/query -H "Content-Type: application/json" -d '{"query": "What is this document about?", "conversation_id": 16, "knowledge_base_id": 59}'

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "answer": "This document discusses a structured approach for controlling robotic systems by breaking complex tasks into smaller sub-problems. Here's an organized summary:\n\n1. **Objective**: The goal is to optimize the movement of robotic systems, particularly end-effector poses, across multiple stages or tasks.\n\n2. **Decomposition**: Each stage of the task is optimized separately. This allows for quick response to disturbances within a single stage while ensuring subsequent stages can react appropriately.\n\n3. **Auxiliary Costs**: Each optimization problem includes costs related to collision avoidance, reachability, path length, and self-collision, ensuring tasks are both accurate and safe.\n\n4. **Bimanual Setup**: The AnyGrasp metric is used to prioritize grasps, determining the effectiveness of each grasp keypoint in picking up objects.\n\n5. **Algorithmic Handling**: The decomposition involves solving sub-problems at 10 Hz using SciPy with Local Optimization, followed by backtracking across stages when constraints are violated.\n\n6. **Keypoint Estimation**: Forward models estimate changes in end-effector poses affecting keypoints, crucial for accurate optimization.\n\n7. **Applications**: This approach is used in real-time control scenarios like autonomous vehicles or industrial robots, ensuring efficient and safe movement across tasks with minimal lag between them.\n\nIn essence, the document presents a method to efficiently manage complex robotic tasks by optimizing each part quickly and reusing solutions where possible, maintaining both performance and safety.",
  "conversation_id": 16,
  "query": {
    "kb_id": 59,
    "original": "What is this document about?"
  },
  "sources": [
    {
      "content": "Two RGB-D cameras, Orbbec Femto Bolt, are mounted on each side of the robot facing the\nworkspace center. The cameras capture RGB images and point clouds at a fixed frequency of 20\nHz.\nA.3 Stationary Dual-Arm Platform\nWe also investigate the method on a stationary dual-arm platform consisting of two Franka arms\nmounted in front of a tabletop workspace (shown in Figure 7). We share the same controller as the\nwheeled single-arm platform with the exception that the two arms are controlled simultaneously at\n20 Hz. Specifically, our pipeline jointly solves two 6-DoF end-effector pose sequences, which are\nsent to the low-level controller together. The controller subsequently calculates IK for both arms\nand moves the arms using joint impedance control.\nThree RGB-D cameras, Orbbec Femto Bolt, are mounted on this platform. Two cameras are\nmounted on the left and right sides and one camera is mounted in the back. The cameras simi-\nlarly capture RGB images and point clouds at a fixed frequency of 20 Hz.\nA.4 Evaluation Details\nBelow we discuss the evaluation details for the experiments reported in Section 4.1 and Section 4.2.\nA.4.1 Details for In-the-Wild and Bimanual Manipulation (Section 4.1)\nFor each task, 10 initial different configurations of objects are selected, which cover the full\nworkspace but are manually verified to ensure they are kinematically feasible for the robot. For\neach trial, a human operator restores the scene to the corresponding configuration and initiates the\nsystem. Due to the challenge of developing automatic success criteria for the diverse set of objects\nand environments investigated in this work, success rates are measured by the operator with the cri-\nterion reported under each task description below. For experiments involving external disturbances,\nthe set of disturbances for all trials is pre-selected, and one disturbance is applied to each trial.\nSpecifically, the disturbance is introduced by a human operator using hands to change the object\u2019s\npose. Collision checking is disabled for all tasks involving deformable objects.\nPour Tea: The environment consists of a teapot and a cup placed on a counter table in a kitchen\nsetting. The task involves three stages: grasping the handle, aligning the teapot to the top of the cup,\nand pouring the tea into the cup. The success criterion requires that the teapot remains upright until\nthe pouring stage, and at the end, the spout must be aligned and tilted on top of the cup opening.\nRecycle Can: The environment includes one of three types of cans (Coke, Zero Coke, Zero Sprite),\na recycle bin with a narrow opening (such that the cans may only go in when they are upright), a\nlandfill bin, and a compost bin, all situated inside an office building. The task involves two stages:\ngrasping the can and reorienting it on top of the recycle bin before dropping it. The success criterion\nis that the can is successfully thrown into the bin.\nStow Book: The environment consists of a target book placed on a side table and a real-size book-\nshelf with a 15cm opening among the placed books, all inside an office environment. The task\ninvolves two stages: grasping the target book on the side and stowing it inside the opening in the\nshelf. The success criterion is that the target book is placed steadily after the robot releases the\ngripper, and the robot must not bump into the shelf or other placed books.\nTape Box: The environment includes a cardboard box, a packaging tape with a dispenser sitting on\ntop of the box that already has one side taped, and a human user collaborating with the robot. The\ntape has already been unrolled to be enough for taping because unrolling typically requires a large\nforce that exceeds the limit of the robot arm. The task involves two stages: while a human operator\nis squeezing the box, the robot needs to grasp the tape and align it to the correct side to complete\nthe taping. The success criterion is that the tape must end up in the correct position such that it is\naligned with the seam.\n21",
      "content_preview": "Two RGB-D cameras, Orbbec Femto Bolt, are mounted on each side of the robot facing the\nworkspace cen...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 20,
        "page_label": "21",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 71.4190579098066
    },
    {
      "content": "arg min\ne1:T ,g1:N\nNX\ni=1\n\uf8ee\n\uf8f0\u03bb(i)\nsub-goal(egi) +\ngiX\nt=gi\u22121\n\u03bb(i)\npath(et)\n\uf8f9\n\uf8fb s.t.\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\ne1 = einit, g0 = 1, 0 < gi < gi+1\nf(kgi) \u2264 0, \u2200f \u2208 C(i)\nsub-goal\nf(kt) \u2264 0, \u2200f \u2208 C(i)\npath, t= gi\u22121, . . . , gi\nkt+1 = h(kt, et), t= 1, . . . , T\u2212 1\n(1)\nwhere et denotes end-effector pose at time t, gi \u2208 {1, . . . , T} are the timings of the transition from\nstage i to i + 1 which are also auxiliary decision variables, kt is the array of keypoint positions at\ntime t, h is a forward model of keypoints, and \u03bb(i)\nsub-goal and \u03bb(i)\npath are auxiliary cost functions (e.g.,\ncollision avoidance) for the sub-goal and path problems respectively. Namely, for each stage i, the\noptimization shall find an end-effector pose as next sub-goal, along with its timing, and a sequence of\nposes egi\u22121:gi that achieves the sub-goal, subject to the given set of ReKep constraints and auxiliary\ncosts. This formulation can be considered as direct shooting in trajectory optimization [124].\n3.3 Decomposition and Algorithmic Instantiation\nTo solve Eq. 1 in real-time, we employ a decomposition of the full problem and only optimize\nfor the immediate next sub-goal and the corresponding path to reach the sub-goal (pseudo-code\nin Algorithm 1). All optimization problems are implemented and solved using SciPy [125] with\ndecision variables normalized to [0, 1]. They are initially solved with Dual Annealing [126] with\nSLSQP [127] as local optimizer (around 1 second) and subsequently solved with only local opti-\nmizer based on the previous solution at approximately 10 Hz1.\nThe Sub-Goal Problem: We first solve the sub-goal problem to obtain egi for the current stage i:\narg min\negi\n\u03bb(i)\nsub-goal(egi) s.t. f(kgi) \u2264 0, \u2200f \u2208 C(i)\nsub-goal (2)\nwhere \u03bbsub-goal subsumes auxiliary control costs: scene collision avoidance, reachability, pose regu-\nlarization, solution consistency, and self-collision for bimanual setup (details in A.8). Namely, Eq. 2\nattempts to find a sub-goal that satisfies Ci\nsub-goal while minimizing the auxiliary costs. If a stage is\nconcerned with grasping, a grasp metric is also included. In this work, we use AnyGrasp [131]2.\nThe Path Problem: After obtaining sub-goalegi, we solve for a trajectoryet:gi starting from current\nend-effector pose et to the sub-goal egi:\narg min\net:gi,gi\n\u03bb(i)\npath(et:gi) s.t. f(k\u02c6t) \u2264 0, \u2200f \u2208 C(i)\npath, \u02c6t = t, . . . , gi (3)\nwhere \u03bbpath subsumes the following auxiliary control costs: scene collision avoidance, reachability,\npath length, solution consistency, and self-collision in the case of bimanual setup (details in A.9). If\nthe distance to the sub-goal egi is within a small tolerance \u03f5, we progress to the next stage i + 1.\nBacktracking: Although the sub-problems can be solved at a real-time frequency to react to external\ndisturbances within a stage, it is imperative that the system can replan across stages if any sub-goal\nconstraint from the last stage no longer holds (e.g., cup taken out of the gripper in the pouring task).\nSpecifically, in every control loop, we check for violation of C(i)\npath. If one is found, we iteratively\nbacktrack to a previous stage j such that C(j)\npath is satisfied.\nForward Models for Keypoints: To solve Eq. 2 and Eq. 3, one must utilize a forward model h\nthat estimates \u2206k from \u2206e in the optimization process. As in prior work [4], we make the rigidity\n1We use finite differencing to obtain the gradients as the solution quality and solving frequency is sufficient\nfor the tasks considered. However, automatic differentiation packages [128\u2013130] may also be used.\n2Since AnyGrasp is a grasp detector instead of a metric and is computationally expensive to run in optimiza-\ntion loops, we always return the grasp closest to a specified \u201cgrasp keypoint\u201d by exploiting the fact that ReKep\nrelated to grasping always associates a dummy keypoint on the end-effector and one actual keypoint.\n5",
      "content_preview": "arg min\ne1:T ,g1:N\nNX\ni=1\n\uf8ee\n\uf8f0\u03bb(i)\nsub-goal(egi) +\ngiX\nt=gi\u22121\n\u03bb(i)\npath(et)\n\uf8f9\n\uf8fb s.t.\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\ne1...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 4,
        "page_label": "5",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 70.36931531182155
    },
    {
      "content": "(Bimanual) Fold Garment: The environment consists of a sweater placed flat close to the\nworkspace center, with small deformations on the sleeves, neck, and bottom. The task typically\nrequires four stages: grasping both sleeves, folding them to the middle, grasping the neck, and fold-\ning it to the bottom. The success criterion does not enforce consistent stages; as long as the sweater\nis folded such that it occupies at most half of the original surface size, it is regarded as a success.\n(Bimanual) Pack Shoes: The environment includes an empty shoe box placed close to the\nworkspace center, with two shoes placed on opposite sides of the box in random poses. The task\ninvolves two stages: grasping the shoes simultaneously and placing them in the shoe box. The suc-\ncess criterion does not enforce consistent stages; as long as the shoes are placed into the box without\nbeing stacked together or causing bimanual self-collision, it is considered successful.\n(Bimanual) Collaborative Folding: The environment consists of a large blanket (pre-folded to an\nappropriate size that occupies about 70% of the workspace due to its size exceeding the workspace\nlimit) and a human user collaborating with the robot. The task involves two stages: the robot must\ngrasp the two corners of the blanket opposite to the human user, and the second stage is aligning\nthe two corners with the two corners that the human has grasped. The success criterion is that the\nrobot has grasped the correct corners and can align them with the correct human arms (left-left,\nright-right).\nA.4.2 Details on Baseline Methods\nWe use V oxPoser [103] as the main baseline method as it makes similar assumptions that no task-\nspecific data or pre-defined motion primitives are required. We adapt V oxPoser to our setting with\ncertain modifications to ensure fairness in comparisons. Specifically, we use the same VLM, GPT-\n4o [6], that takes the same camera input. We also augment the original prompt from the paper with\nthe prompt used in this work to ensure it has sufficient context. We only use the affordance, rotation,\nand gripper action value maps and ignore the avoidance and velocity value maps because they are\nnot necessary for our tasks. We also only consider the scenario where the \u201centity of interest\u201d is\nthe robot end-effector instead of objects in the scene. The latter is tailored for pushing task, which\nis not being studied in this work. We use OWL-ViT [135] for open-vocabulary object detection,\nSAM [132] for initial-frame segmentation, and Cutie [136] for mask tracking.\nA.4.3 Details for Generalization in Manipulation Strategies (Section 4.2)\nThe dual-arm robot is tasked with folding eight different categories of clothing. We use two metrics\nfor evaluation: \u201cStrategy Success\u201d and \u201cExecution Success,\u201d where the former evaluates whether\nkeypoints are proposed and constraints are written appropriately, and the latter evaluates the robotic\nsystem\u2019s execution given successful strategies.\nTo evaluate \u201cStrategy Success,\u201d the garment is initialized close to the center of the workspace. A\nback-mounted RGB-D camera captures the RGB image. Then, the keypoint proposal module gen-\nerates keypoint candidates using the captured image, which are then overlaid on top of the original\nimage with numerical marks {0, . . . , K\u2212 1}. The overlaid image, along with the same generic\nprompt, is fed into GPT-4 [6] to generate the ReKep constraints. Since folding garments is itself an\nopen-ended problem without ground-truth strategies, we manually judge if the proposed keypoints\nand the generated constraints are correct. Note that since the constraints are to be executed by a\nbimanual robot, and the constraints are almost always connecting (folding) two keypoints such that\nthey are aligned, correctness is measured by whether it is (potentially) executable by the robot with-\nout causing self-collision (arms crossing over to opposite sides) and whether the folding strategy can\nfold the garment to at most half of its original surface area.\nTo evaluate \u201cExecution Success,\u201d we take the generated strategies in the previous section that are\nmarked as successful for each garment and execute the sequence on the dual-arm platform, with a\ntotal of 10 trials for each garment. Point tracking is disabled as we observe that our point tracker\npredicts unstable tracks when the garment is potentially folded many times. Success is measured by\nwhether the garment is folded such that its surface area is at most half of its original surface area.\n22",
      "content_preview": "(Bimanual) Fold Garment: The environment consists of a sweater placed flat close to the\nworkspace ce...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 21,
        "page_label": "22",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 70.06284945589441
    },
    {
      "content": "Path Length: We approximate the path length using the dense samples by taking the sum of their\ndifferences. Shorter paths are encouraged.\nReachability: We solve an IK problem for each intermediate pose inside the objective function as\nin the sub-goal problem. See the sub-goal solver section for more details.\nConsistency: As in the sub-goal problem, we encourage the solution to be close to the previous\none. Specifically, we store the dense samples from the previous iteration. To calculate the solution\nconsistency, we use the pairwise distance between the two sequences (treated as two sets) as an\nefficient proxy. Alternatively, Hausdorff distance can be used.\n(Dual-Arm only) Self-Collision Avoidance: We similarly compute self-collision avoidance for the\ndual-arm platform as in the sub-goal problem. We also use pairwise distance between the two\nsequences to efficiently calculate this cost.\nA.10 Comparisons with Prior Works on Visual Prompting for Manipulation\nThere has been several concurrent works investigating the application of visual prompting of VLMs\nto robotic manipulation [96\u201398, 109, 122]. Below we summarized the differences to highlight the\ncontributions of this work.\nTask DoF: In this work, we focus on challenging tasks that require 6 DoF (single arm) or 12 DoF\n(two arms) motions. However, this is not trivial for existing VLMs which operate on 2D images \u2013\nas quoted from MOKA [97], \u201ccurrent VLMs are not capable of reliably predicting 6-DoF motions\u201d\nand PIVOT [98], \u201cgeneralizing to higher dimensional spaces such as rotation poses even additional\nchallenges\u201d. To tackle this, one key insight from ReKep is that VLMs only need to implicitly specify\nfull 3D rotations by reasoning about keypoints in (x, y, z) Cartesian coordinates. After this, actual\n3D rotations are solved by high-precision and efficient numerical solvers, effectively sidestepping\nthe challenge of explicitly predicting 3D rotations. As a result, the same formulation also naturally\ngeneralizes to controlling multiple arms.\nHigh-Level Planning: While many works also consider multi-stage tasks via an language-based\ntask planners which are independent from their methods, our formulation takes inspiration from\nTAMP and organically integrates high-level task planning with low-level actions in a unified contin-\nuous mathematical program. As a result, the method can naturally consider geometric dependencies\nacross stages and do so at a real-time frequency . When a failure occurs, it would backtrack to a\nprevious stage in which its conditions can still be satisfied. For example, in the \u201cpouring tea\u201d task,\nthe robot can only start tilting the teapot when the teapot spout is aligned with the cup opening.\nHowever, if the cup is being moved in the process, it should level the teapot and re-align with the\ncup. Or if the teapot is being taken from the gripper, it should instead re-grasp the teapot.\nLow-Level Execution: A common issue with using VLMs is that it is computationally expensive\nto run, hindering the high-frequency perception-action feedback loops often required for many ma-\nnipulation tasks. As a result, most of existing works either consider the open-loop settings where\nvisual perception is only used in the beginning or only consider the tasks where slow execution is\nacceptable. Instead, our formulation natively supports a high-frequency perception-action loops by\ncoupling VLMs with a point tracker, which effectively enables reactive behaviors via closed-loop\nexecution despite leveraging very large foundation models.\nVisual Prompting Methods: We uniquely consider using visual prompting for code-generation,\nwhere code may contain arbitrary arithmetic operations on a set of keypoints via visual referring\nexpressions. Although a single point is limiting to capture complex geometric structure, multiple\npoints and their relations can even specify vectors, surfaces, volumes, and their temporal dependen-\ncies. While being conceptually simple, this offers a much higher degree of flexibility which can\nfully specify 6 DoF or even 12 DoF motions.\n27",
      "content_preview": "Path Length: We approximate the path length using the dense samples by taking the sum of their\ndiffe...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 26,
        "page_label": "27",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 65.03731640945789
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:02
CURL COMMAND:
curl -X GET http://localhost:8080/conversations

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "conversations": [
    {
      "created_at": "2025-03-26 20:20:31",
      "id": 16,
      "knowledge_base_id": 59,
      "last_message": {
        "content": "What is this document about?",
        "conversation_id": 16,
        "created_at": "2025-03-26 20:21:02",
        "id": 37,
        "message_type": "user",
        "sources": null
      },
      "title": "Test Conversation",
      "updated_at": "2025-03-26 20:21:02"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:02
CURL COMMAND:
curl -X GET http://localhost:8080/conversations

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "conversations": [
    {
      "created_at": "2025-03-26 20:20:31",
      "id": 16,
      "knowledge_base_id": 59,
      "last_message": {
        "content": "What is this document about?",
        "conversation_id": 16,
        "created_at": "2025-03-26 20:21:02",
        "id": 37,
        "message_type": "user",
        "sources": null
      },
      "title": "Test Conversation",
      "updated_at": "2025-03-26 20:21:02"
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:02
CURL COMMAND:
curl -X GET http://localhost:8080/conversations/16

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "created_at": "2025-03-26 20:20:31",
  "id": 16,
  "knowledge_base_id": 59,
  "messages": [
    {
      "content": "What is this document about?",
      "conversation_id": 16,
      "created_at": "2025-03-26 20:21:02",
      "id": 37,
      "message_type": "user",
      "sources": null
    },
    {
      "content": "This document discusses a structured approach for controlling robotic systems by breaking complex tasks into smaller sub-problems. Here's an organized summary:\n\n1. **Objective**: The goal is to optimize the movement of robotic systems, particularly end-effector poses, across multiple stages or tasks.\n\n2. **Decomposition**: Each stage of the task is optimized separately. This allows for quick response to disturbances within a single stage while ensuring subsequent stages can react appropriately.\n\n3. **Auxiliary Costs**: Each optimization problem includes costs related to collision avoidance, reachability, path length, and self-collision, ensuring tasks are both accurate and safe.\n\n4. **Bimanual Setup**: The AnyGrasp metric is used to prioritize grasps, determining the effectiveness of each grasp keypoint in picking up objects.\n\n5. **Algorithmic Handling**: The decomposition involves solving sub-problems at 10 Hz using SciPy with Local Optimization, followed by backtracking across stages when constraints are violated.\n\n6. **Keypoint Estimation**: Forward models estimate changes in end-effector poses affecting keypoints, crucial for accurate optimization.\n\n7. **Applications**: This approach is used in real-time control scenarios like autonomous vehicles or industrial robots, ensuring efficient and safe movement across tasks with minimal lag between them.\n\nIn essence, the document presents a method to efficiently manage complex robotic tasks by optimizing each part quickly and reusing solutions where possible, maintaining both performance and safety.",
      "conversation_id": 16,
      "created_at": "2025-03-26 20:21:02",
      "id": 38,
      "message_type": "assistant",
      "sources": "[{\"document_name\": \"rekep.pdf\", \"content\": \"Two RGB-D cameras, Orbbec Femto Bolt, are mounted on each side of the robot facing the\\nworkspace center. The cameras capture RGB images and point clouds at a fixed frequency of 20\\nHz.\\nA.3 Stationary Dual-Arm Platform\\nWe also investigate the method on a stationary dual-arm platform consisting of two Franka arms\\nmounted in front of a tabletop workspace (shown in Figure 7). We share the same controller as the\\nwheeled single-arm platform with the exception that the two arms are controlled simultaneously at\\n20 Hz. Specifically, our pipeline jointly solves two 6-DoF end-effector pose sequences, which are\\nsent to the low-level controller together. The controller subsequently calculates IK for both arms\\nand moves the arms using joint impedance control.\\nThree RGB-D cameras, Orbbec Femto Bolt, are mounted on this platform. Two cameras are\\nmounted on the left and right sides and one camera is mounted in the back. The cameras simi-\\nlarly capture RGB images and point clouds at a fixed frequency of 20 Hz.\\nA.4 Evaluation Details\\nBelow we discuss the evaluation details for the experiments reported in Section 4.1 and Section 4.2.\\nA.4.1 Details for In-the-Wild and Bimanual Manipulation (Section 4.1)\\nFor each task, 10 initial different configurations of objects are selected, which cover the full\\nworkspace but are manually verified to ensure they are kinematically feasible for the robot. For\\neach trial, a human operator restores the scene to the corresponding configuration and initiates the\\nsystem. Due to the challenge of developing automatic success criteria for the diverse set of objects\\nand environments investigated in this work, success rates are measured by the operator with the cri-\\nterion reported under each task description below. For experiments involving external disturbances,\\nthe set of disturbances for all trials is pre-selected, and one disturbance is applied to each trial.\\nSpecifically, the disturbance is introduced by a human operator using hands to change the object\\u2019s\\npose. Collision checking is disabled for all tasks involving deformable objects.\\nPour Tea: The environment consists of a teapot and a cup placed on a counter table in a kitchen\\nsetting. The task involves three stages: grasping the handle, aligning the teapot to the top of the cup,\\nand pouring the tea into the cup. The success criterion requires that the teapot remains upright until\\nthe pouring stage, and at the end, the spout must be aligned and tilted on top of the cup opening.\\nRecycle Can: The environment includes one of three types of cans (Coke, Zero Coke, Zero Sprite),\\na recycle bin with a narrow opening (such that the cans may only go in when they are upright), a\\nlandfill bin, and a compost bin, all situated inside an office building. The task involves two stages:\\ngrasping the can and reorienting it on top of the recycle bin before dropping it. The success criterion\\nis that the can is successfully thrown into the bin.\\nStow Book: The environment consists of a target book placed on a side table and a real-size book-\\nshelf with a 15cm opening among the placed books, all inside an office environment. The task\\ninvolves two stages: grasping the target book on the side and stowing it inside the opening in the\\nshelf. The success criterion is that the target book is placed steadily after the robot releases the\\ngripper, and the robot must not bump into the shelf or other placed books.\\nTape Box: The environment includes a cardboard box, a packaging tape with a dispenser sitting on\\ntop of the box that already has one side taped, and a human user collaborating with the robot. The\\ntape has already been unrolled to be enough for taping because unrolling typically requires a large\\nforce that exceeds the limit of the robot arm. The task involves two stages: while a human operator\\nis squeezing the box, the robot needs to grasp the tape and align it to the correct side to complete\\nthe taping. The success criterion is that the tape must end up in the correct position such that it is\\naligned with the seam.\\n21\", \"content_preview\": \"Two RGB-D cameras, Orbbec Femto Bolt, are mounted on each side of the robot facing the\\nworkspace cen...\", \"relevance_score\": 71.4190579098066, \"metadata\": {\"author\": \"Anonymous Submission\", \"creationdate\": \"2024-09-03T06:36:35+00:00\", \"creator\": \"LaTeX with hyperref\", \"keywords\": \"\", \"moddate\": \"2024-09-03T06:36:35+00:00\", \"page\": 20, \"page_label\": \"21\", \"producer\": \"pdfTeX-1.40.25\", \"ptex.fullbanner\": \"This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\", \"subject\": \"Proceedings of the 8th Conference on Robot Learning (CoRL 2024)\", \"title\": \"\", \"total_pages\": 31, \"trapped\": \"/False\"}}, {\"document_name\": \"rekep.pdf\", \"content\": \"arg min\\ne1:T ,g1:N\\nNX\\ni=1\\n\\uf8ee\\n\\uf8f0\\u03bb(i)\\nsub-goal(egi) +\\ngiX\\nt=gi\\u22121\\n\\u03bb(i)\\npath(et)\\n\\uf8f9\\n\\uf8fb s.t.\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\ne1 = einit, g0 = 1, 0 < gi < gi+1\\nf(kgi) \\u2264 0, \\u2200f \\u2208 C(i)\\nsub-goal\\nf(kt) \\u2264 0, \\u2200f \\u2208 C(i)\\npath, t= gi\\u22121, . . . , gi\\nkt+1 = h(kt, et), t= 1, . . . , T\\u2212 1\\n(1)\\nwhere et denotes end-effector pose at time t, gi \\u2208 {1, . . . , T} are the timings of the transition from\\nstage i to i + 1 which are also auxiliary decision variables, kt is the array of keypoint positions at\\ntime t, h is a forward model of keypoints, and \\u03bb(i)\\nsub-goal and \\u03bb(i)\\npath are auxiliary cost functions (e.g.,\\ncollision avoidance) for the sub-goal and path problems respectively. Namely, for each stage i, the\\noptimization shall find an end-effector pose as next sub-goal, along with its timing, and a sequence of\\nposes egi\\u22121:gi that achieves the sub-goal, subject to the given set of ReKep constraints and auxiliary\\ncosts. This formulation can be considered as direct shooting in trajectory optimization [124].\\n3.3 Decomposition and Algorithmic Instantiation\\nTo solve Eq. 1 in real-time, we employ a decomposition of the full problem and only optimize\\nfor the immediate next sub-goal and the corresponding path to reach the sub-goal (pseudo-code\\nin Algorithm 1). All optimization problems are implemented and solved using SciPy [125] with\\ndecision variables normalized to [0, 1]. They are initially solved with Dual Annealing [126] with\\nSLSQP [127] as local optimizer (around 1 second) and subsequently solved with only local opti-\\nmizer based on the previous solution at approximately 10 Hz1.\\nThe Sub-Goal Problem: We first solve the sub-goal problem to obtain egi for the current stage i:\\narg min\\negi\\n\\u03bb(i)\\nsub-goal(egi) s.t. f(kgi) \\u2264 0, \\u2200f \\u2208 C(i)\\nsub-goal (2)\\nwhere \\u03bbsub-goal subsumes auxiliary control costs: scene collision avoidance, reachability, pose regu-\\nlarization, solution consistency, and self-collision for bimanual setup (details in A.8). Namely, Eq. 2\\nattempts to find a sub-goal that satisfies Ci\\nsub-goal while minimizing the auxiliary costs. If a stage is\\nconcerned with grasping, a grasp metric is also included. In this work, we use AnyGrasp [131]2.\\nThe Path Problem: After obtaining sub-goalegi, we solve for a trajectoryet:gi starting from current\\nend-effector pose et to the sub-goal egi:\\narg min\\net:gi,gi\\n\\u03bb(i)\\npath(et:gi) s.t. f(k\\u02c6t) \\u2264 0, \\u2200f \\u2208 C(i)\\npath, \\u02c6t = t, . . . , gi (3)\\nwhere \\u03bbpath subsumes the following auxiliary control costs: scene collision avoidance, reachability,\\npath length, solution consistency, and self-collision in the case of bimanual setup (details in A.9). If\\nthe distance to the sub-goal egi is within a small tolerance \\u03f5, we progress to the next stage i + 1.\\nBacktracking: Although the sub-problems can be solved at a real-time frequency to react to external\\ndisturbances within a stage, it is imperative that the system can replan across stages if any sub-goal\\nconstraint from the last stage no longer holds (e.g., cup taken out of the gripper in the pouring task).\\nSpecifically, in every control loop, we check for violation of C(i)\\npath. If one is found, we iteratively\\nbacktrack to a previous stage j such that C(j)\\npath is satisfied.\\nForward Models for Keypoints: To solve Eq. 2 and Eq. 3, one must utilize a forward model h\\nthat estimates \\u2206k from \\u2206e in the optimization process. As in prior work [4], we make the rigidity\\n1We use finite differencing to obtain the gradients as the solution quality and solving frequency is sufficient\\nfor the tasks considered. However, automatic differentiation packages [128\\u2013130] may also be used.\\n2Since AnyGrasp is a grasp detector instead of a metric and is computationally expensive to run in optimiza-\\ntion loops, we always return the grasp closest to a specified \\u201cgrasp keypoint\\u201d by exploiting the fact that ReKep\\nrelated to grasping always associates a dummy keypoint on the end-effector and one actual keypoint.\\n5\", \"content_preview\": \"arg min\\ne1:T ,g1:N\\nNX\\ni=1\\n\\uf8ee\\n\\uf8f0\\u03bb(i)\\nsub-goal(egi) +\\ngiX\\nt=gi\\u22121\\n\\u03bb(i)\\npath(et)\\n\\uf8f9\\n\\uf8fb s.t.\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\ne1...\", \"relevance_score\": 70.36931531182155, \"metadata\": {\"author\": \"Anonymous Submission\", \"creationdate\": \"2024-09-03T06:36:35+00:00\", \"creator\": \"LaTeX with hyperref\", \"keywords\": \"\", \"moddate\": \"2024-09-03T06:36:35+00:00\", \"page\": 4, \"page_label\": \"5\", \"producer\": \"pdfTeX-1.40.25\", \"ptex.fullbanner\": \"This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\", \"subject\": \"Proceedings of the 8th Conference on Robot Learning (CoRL 2024)\", \"title\": \"\", \"total_pages\": 31, \"trapped\": \"/False\"}}, {\"document_name\": \"rekep.pdf\", \"content\": \"(Bimanual) Fold Garment: The environment consists of a sweater placed flat close to the\\nworkspace center, with small deformations on the sleeves, neck, and bottom. The task typically\\nrequires four stages: grasping both sleeves, folding them to the middle, grasping the neck, and fold-\\ning it to the bottom. The success criterion does not enforce consistent stages; as long as the sweater\\nis folded such that it occupies at most half of the original surface size, it is regarded as a success.\\n(Bimanual) Pack Shoes: The environment includes an empty shoe box placed close to the\\nworkspace center, with two shoes placed on opposite sides of the box in random poses. The task\\ninvolves two stages: grasping the shoes simultaneously and placing them in the shoe box. The suc-\\ncess criterion does not enforce consistent stages; as long as the shoes are placed into the box without\\nbeing stacked together or causing bimanual self-collision, it is considered successful.\\n(Bimanual) Collaborative Folding: The environment consists of a large blanket (pre-folded to an\\nappropriate size that occupies about 70% of the workspace due to its size exceeding the workspace\\nlimit) and a human user collaborating with the robot. The task involves two stages: the robot must\\ngrasp the two corners of the blanket opposite to the human user, and the second stage is aligning\\nthe two corners with the two corners that the human has grasped. The success criterion is that the\\nrobot has grasped the correct corners and can align them with the correct human arms (left-left,\\nright-right).\\nA.4.2 Details on Baseline Methods\\nWe use V oxPoser [103] as the main baseline method as it makes similar assumptions that no task-\\nspecific data or pre-defined motion primitives are required. We adapt V oxPoser to our setting with\\ncertain modifications to ensure fairness in comparisons. Specifically, we use the same VLM, GPT-\\n4o [6], that takes the same camera input. We also augment the original prompt from the paper with\\nthe prompt used in this work to ensure it has sufficient context. We only use the affordance, rotation,\\nand gripper action value maps and ignore the avoidance and velocity value maps because they are\\nnot necessary for our tasks. We also only consider the scenario where the \\u201centity of interest\\u201d is\\nthe robot end-effector instead of objects in the scene. The latter is tailored for pushing task, which\\nis not being studied in this work. We use OWL-ViT [135] for open-vocabulary object detection,\\nSAM [132] for initial-frame segmentation, and Cutie [136] for mask tracking.\\nA.4.3 Details for Generalization in Manipulation Strategies (Section 4.2)\\nThe dual-arm robot is tasked with folding eight different categories of clothing. We use two metrics\\nfor evaluation: \\u201cStrategy Success\\u201d and \\u201cExecution Success,\\u201d where the former evaluates whether\\nkeypoints are proposed and constraints are written appropriately, and the latter evaluates the robotic\\nsystem\\u2019s execution given successful strategies.\\nTo evaluate \\u201cStrategy Success,\\u201d the garment is initialized close to the center of the workspace. A\\nback-mounted RGB-D camera captures the RGB image. Then, the keypoint proposal module gen-\\nerates keypoint candidates using the captured image, which are then overlaid on top of the original\\nimage with numerical marks {0, . . . , K\\u2212 1}. The overlaid image, along with the same generic\\nprompt, is fed into GPT-4 [6] to generate the ReKep constraints. Since folding garments is itself an\\nopen-ended problem without ground-truth strategies, we manually judge if the proposed keypoints\\nand the generated constraints are correct. Note that since the constraints are to be executed by a\\nbimanual robot, and the constraints are almost always connecting (folding) two keypoints such that\\nthey are aligned, correctness is measured by whether it is (potentially) executable by the robot with-\\nout causing self-collision (arms crossing over to opposite sides) and whether the folding strategy can\\nfold the garment to at most half of its original surface area.\\nTo evaluate \\u201cExecution Success,\\u201d we take the generated strategies in the previous section that are\\nmarked as successful for each garment and execute the sequence on the dual-arm platform, with a\\ntotal of 10 trials for each garment. Point tracking is disabled as we observe that our point tracker\\npredicts unstable tracks when the garment is potentially folded many times. Success is measured by\\nwhether the garment is folded such that its surface area is at most half of its original surface area.\\n22\", \"content_preview\": \"(Bimanual) Fold Garment: The environment consists of a sweater placed flat close to the\\nworkspace ce...\", \"relevance_score\": 70.06284945589441, \"metadata\": {\"author\": \"Anonymous Submission\", \"creationdate\": \"2024-09-03T06:36:35+00:00\", \"creator\": \"LaTeX with hyperref\", \"keywords\": \"\", \"moddate\": \"2024-09-03T06:36:35+00:00\", \"page\": 21, \"page_label\": \"22\", \"producer\": \"pdfTeX-1.40.25\", \"ptex.fullbanner\": \"This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\", \"subject\": \"Proceedings of the 8th Conference on Robot Learning (CoRL 2024)\", \"title\": \"\", \"total_pages\": 31, \"trapped\": \"/False\"}}, {\"document_name\": \"rekep.pdf\", \"content\": \"Path Length: We approximate the path length using the dense samples by taking the sum of their\\ndifferences. Shorter paths are encouraged.\\nReachability: We solve an IK problem for each intermediate pose inside the objective function as\\nin the sub-goal problem. See the sub-goal solver section for more details.\\nConsistency: As in the sub-goal problem, we encourage the solution to be close to the previous\\none. Specifically, we store the dense samples from the previous iteration. To calculate the solution\\nconsistency, we use the pairwise distance between the two sequences (treated as two sets) as an\\nefficient proxy. Alternatively, Hausdorff distance can be used.\\n(Dual-Arm only) Self-Collision Avoidance: We similarly compute self-collision avoidance for the\\ndual-arm platform as in the sub-goal problem. We also use pairwise distance between the two\\nsequences to efficiently calculate this cost.\\nA.10 Comparisons with Prior Works on Visual Prompting for Manipulation\\nThere has been several concurrent works investigating the application of visual prompting of VLMs\\nto robotic manipulation [96\\u201398, 109, 122]. Below we summarized the differences to highlight the\\ncontributions of this work.\\nTask DoF: In this work, we focus on challenging tasks that require 6 DoF (single arm) or 12 DoF\\n(two arms) motions. However, this is not trivial for existing VLMs which operate on 2D images \\u2013\\nas quoted from MOKA [97], \\u201ccurrent VLMs are not capable of reliably predicting 6-DoF motions\\u201d\\nand PIVOT [98], \\u201cgeneralizing to higher dimensional spaces such as rotation poses even additional\\nchallenges\\u201d. To tackle this, one key insight from ReKep is that VLMs only need to implicitly specify\\nfull 3D rotations by reasoning about keypoints in (x, y, z) Cartesian coordinates. After this, actual\\n3D rotations are solved by high-precision and efficient numerical solvers, effectively sidestepping\\nthe challenge of explicitly predicting 3D rotations. As a result, the same formulation also naturally\\ngeneralizes to controlling multiple arms.\\nHigh-Level Planning: While many works also consider multi-stage tasks via an language-based\\ntask planners which are independent from their methods, our formulation takes inspiration from\\nTAMP and organically integrates high-level task planning with low-level actions in a unified contin-\\nuous mathematical program. As a result, the method can naturally consider geometric dependencies\\nacross stages and do so at a real-time frequency . When a failure occurs, it would backtrack to a\\nprevious stage in which its conditions can still be satisfied. For example, in the \\u201cpouring tea\\u201d task,\\nthe robot can only start tilting the teapot when the teapot spout is aligned with the cup opening.\\nHowever, if the cup is being moved in the process, it should level the teapot and re-align with the\\ncup. Or if the teapot is being taken from the gripper, it should instead re-grasp the teapot.\\nLow-Level Execution: A common issue with using VLMs is that it is computationally expensive\\nto run, hindering the high-frequency perception-action feedback loops often required for many ma-\\nnipulation tasks. As a result, most of existing works either consider the open-loop settings where\\nvisual perception is only used in the beginning or only consider the tasks where slow execution is\\nacceptable. Instead, our formulation natively supports a high-frequency perception-action loops by\\ncoupling VLMs with a point tracker, which effectively enables reactive behaviors via closed-loop\\nexecution despite leveraging very large foundation models.\\nVisual Prompting Methods: We uniquely consider using visual prompting for code-generation,\\nwhere code may contain arbitrary arithmetic operations on a set of keypoints via visual referring\\nexpressions. Although a single point is limiting to capture complex geometric structure, multiple\\npoints and their relations can even specify vectors, surfaces, volumes, and their temporal dependen-\\ncies. While being conceptually simple, this offers a much higher degree of flexibility which can\\nfully specify 6 DoF or even 12 DoF motions.\\n27\", \"content_preview\": \"Path Length: We approximate the path length using the dense samples by taking the sum of their\\ndiffe...\", \"relevance_score\": 65.03731640945789, \"metadata\": {\"author\": \"Anonymous Submission\", \"creationdate\": \"2024-09-03T06:36:35+00:00\", \"creator\": \"LaTeX with hyperref\", \"keywords\": \"\", \"moddate\": \"2024-09-03T06:36:35+00:00\", \"page\": 26, \"page_label\": \"27\", \"producer\": \"pdfTeX-1.40.25\", \"ptex.fullbanner\": \"This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\", \"subject\": \"Proceedings of the 8th Conference on Robot Learning (CoRL 2024)\", \"title\": \"\", \"total_pages\": 31, \"trapped\": \"/False\"}}]"
    }
  ],
  "title": "Test Conversation",
  "updated_at": "2025-03-26 20:21:02"
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:02
CURL COMMAND:
curl -X POST http://localhost:8080/conversations/16/messages -H "Content-Type: application/json" -d '{"content": "This is a test message added directly", "message_type": "user"}'

RESPONSE STATUS: 201
RESPONSE BODY:
{
  "message": "message added",
  "message_id": 39
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:28
CURL COMMAND:
curl -X POST http://localhost:8080/query -H "Content-Type: application/json" -d '{"query": "Summarize this document briefly", "knowledge_base_id": 59}'

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "answer": "Summarized content:\n\nThis document discusses human-centered approaches in robotic manipulation using ReKEP (Relational Keypoint Constraints). The authors propose an integrated framework for robotic manipulation tasks with visual inputs, focusing on task and motion planning under uncertainty. They introduce a constrained optimization problem that includes both geometric and relational constraints, formulated as mathematical programs that are repeatedly solved in a receding-horizon fashion. ReKEP is designed to handle multiple degrees of freedom, integrate reasoning modules for high-level planning, and generate black-box constraints via visual prompting. The paper contributes to advancing human-centered robotic manipulation by providing an efficient and scalable solution for complex manipulation tasks.",
  "query": {
    "kb_id": 59,
    "original": "Summarize this document briefly"
  },
  "sources": [
    {
      "content": "depth maps from all available cameras and excludes robot arms using cuRobo and any grasped rigid\nobjects (tracked via a masked tracker model Cutie [136]). A collision voxel grid is then calculated\nusing the ESDF and used by other modules in the system. In the sub-goal solver module, we first\ndownsample the gripper points and the grasped object points to have a maximum of 30 points using\nfarthest point sampling. Then we calculate the collision cost using the ESDF voxel grid with linear\ninterpolation with a threshold of 15cm.\nReachability: Since our decision variables are end-effector poses, which may not be always reach-\nable by the robot arms, especially in confined spaces, we need to add a cost term that encourages\nfinding solutions with valid joint configurations. Therefore, we solve an IK problem in each iteration\nof the sub-goal solver using PyBullet [133] and use its residual as a proxy for reachability. We find\nthat this takes around 40% of the time of the full objective function. Alternatively, one may solve\nthe problem in joint space, which would ensure the solution is within the joint limits by enforcing\nthe bounds. We find that this is inefficient with our Python-based implementation as we need to cal-\nculate forward kinematics for a magnitude of more times in the path solver, because the constraints\nare evaluated in the task space. To address this while ensuring efficiency, future works can consider\nusing hardware-accelerated implementations to solve the problems in joint space [58].\nPose Regularization: We also add a small cost that encourages the sub-goal to be close to the\ncurrent end-effector pose.\nConsistency: Since the solver iteratively solves the problem at a high frequency and the noise from\nthe perception pipeline may propagate to the solver, we find it useful to include a consistency cost\nthat encourages the solution to be close to the previous solution.\n(Dual-Arm only) Self-Collision Avoidance: To avoid two arms collide with each other, we compute\nthe pairwise distance between the two point sets, each including the gripper points and grasped\nobject points.\nA.9 Implementation Details of Path Solver\nThe path problems are implemented and solved using SciPy [125]. The number of decision variables\nis calculated based on the distance between the current end-effector pose and the target end-effector\npose. Specifically, we define a fixed step size ( 20cm and 45 degree) and linearly approximate the\ndesired number of \u201cintermediate poses\u201d, which are used as decision variables. As in the sub-goal\nproblem, they are similarly represented using position and Euler angles with the same bounds. For\nthe first solving iteration, the initial guess is chosen to be linear interpolation between the start and\nthe target. We similarly use sampling-based global optimization followed by a gradient-based local\noptimizer in the first iteration and only use local optimizer in subsequent iterations. After we obtain\nthe solution, represented as a number of intermediate poses, we fit a spline using the current pose,\nthe intermediate poses, and the target pose, which are then densely sampled to be executed by the\nrobot.\nIn the objective function, we first unnormalize the decision variables and use piecewise linear in-\nterpolation to obtain a dense sequence of discrete poses to represent the path (referred to as \u201cdense\nsamples\u201d below). A spline interpolation would be aligned with how we postprocess and execute the\nsolution, but we find linear interpolation to be computationally more efficient. Below we discuss the\nindividual cost terms in the objective function.\nConstraint Violation: Similar to that in the sub-goal problem, we check violation of the ReKep\nconstraints for each dense sample along the path and penalize with large weights.\nScene Collision Avoidance: The calculation is similar to the sub-goal problem, except that it is\ncalculated for each dense sample. We ignore the collision calculation with a 5cm radius near the\nstart and the target poses, as this tends to stabilize the solution when solved at a high frequency due\nto various real-world noises. We additionally add a table clearance cost that penalizes the path from\npenetrating the table (or the bottom of the workspace for the wheeled single-arm robot).\n26",
      "content_preview": "depth maps from all available cameras and excludes robot arms using cuRobo and any grasped rigid\nobj...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 25,
        "page_label": "26",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 70.34661468773784
    },
    {
      "content": "A.11 Extended Discusssions on Limitations\nHerein we present additional limitations of the existing system.\nPrompting and Robustness: Although we have demonstrated that existing VLMs possess rudi-\nmentary capabilities at specifying ReKep constraints, we have observed that when dealing with\ntasks that span many stages with several temporally dependent constraints (A.14), the VLMs lack\nenough robustness to obtain consistent success.\nTask-Space Planning: To enable efficient planning, in this work we only consider planning in the\ntask space with the end-effector poses as decision variables. However, we have observed that in\ncertain scenarios, it may be kinematically challenging for robots to achieve the optimized poses as\nthe solver does not explicitly account for the kinematics of the robot. Planning in joint space can\nlikely resolve the issue but we find it to be less computationally efficient for our tasks.\nArticulated Object Manipulation: In this work, we do not investigate tasks involving articulated\nobjects, as we observed this requires advanced spatial reasoning capabilities that are beyond those\nof existing VLMs. However, the ReKep formulation may be extended to such tasks by representing\ndifferent types of joints also by \u201crelations of keypoints\u201d. For example, ReKep constraints can be\nwritten to constrain certain keypoints to move only alongside a line (prismatic joints) or a curve\n(revolute joints). To extend to these scenarios, finetuning may be required as in [150\u2013152].\nBimanual Coordination: Although we demonstrate the application of ReKep to bimanual manipu-\nlation, we also identify several important limitations in this domain. Notably, the challenges can be\nroughly categorized into those pertaining to semantic reasoning of keypoint relations by the VLM\nand those pertaining to solving for bimanual motions by the optimization solver. For semantic rea-\nsoning, to achieve bimanual folding, the VLM needs to possess certain spatial knowledge about\nwhich steps should/can be performed together by both arms. For example, the bottom of a shirt\noften needs to be grasped by two hands, each at one corner, in order to fold it upwards to align with\nthe collar. Another example in blanket folding is to recognize that the bottom-left corner should\nbe aligned with the top-left corner and the bottom-right corner should be aligned with the top-right\ncorner, as other matching may lead to self-collision. For optimization solver, as bimanual motion\nplanning dramatically increases the search space of possible motions, which slows down the overall\npipeline and more frequently produces less optimal behaviors.\nA.12 Simulation Experiments\nWe additionally implement ReKep in OmniGibson [153] for the Pour Tea task. It is compared\nto a monolithic learning-based baseline based on the transformer architecture [154] adopted from\nRVT [155, 156]. The baseline is trained via imitation learning on 100 expert demonstrations, where\ndemonstrations are from scripted policies using privileged simulation information. Success rates are\naveraged across 100 trials and reported below. Although the monolithic policy excels in training\nscenarios given its access to expert demonstrations, we observe that ReKep performs significantly\nstronger in unseen settings, and more importantly, without the need of expert demonstrations.\nSeen Poses Unseen Poses Unseen Objects\nMonolithic Policy 0.93 0.31 0.14\nReKep (Zero-Shot) 0.75 0.68 0.72\nA.13 Comparisons of Visual Feature Extractors for Keypoint Proposal\nHerein we provide qualitative comparisons of different methods for keypoint proposal. We compare\nthree pre-trained visual feature extractors, each of which represents a popular class of pre-training\nmethods: DINOv2 [5] (self-supervised pre-training), CLIP [91] (vision-language contrastive pre-\ntraining), and ViT [157] (supervised pre-training). We also compare to a variant that does not use\n28",
      "content_preview": "A.11 Extended Discusssions on Limitations\nHerein we present additional limitations of the existing s...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 27,
        "page_label": "28",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 68.66986601101394
    },
    {
      "content": "Path Length: We approximate the path length using the dense samples by taking the sum of their\ndifferences. Shorter paths are encouraged.\nReachability: We solve an IK problem for each intermediate pose inside the objective function as\nin the sub-goal problem. See the sub-goal solver section for more details.\nConsistency: As in the sub-goal problem, we encourage the solution to be close to the previous\none. Specifically, we store the dense samples from the previous iteration. To calculate the solution\nconsistency, we use the pairwise distance between the two sequences (treated as two sets) as an\nefficient proxy. Alternatively, Hausdorff distance can be used.\n(Dual-Arm only) Self-Collision Avoidance: We similarly compute self-collision avoidance for the\ndual-arm platform as in the sub-goal problem. We also use pairwise distance between the two\nsequences to efficiently calculate this cost.\nA.10 Comparisons with Prior Works on Visual Prompting for Manipulation\nThere has been several concurrent works investigating the application of visual prompting of VLMs\nto robotic manipulation [96\u201398, 109, 122]. Below we summarized the differences to highlight the\ncontributions of this work.\nTask DoF: In this work, we focus on challenging tasks that require 6 DoF (single arm) or 12 DoF\n(two arms) motions. However, this is not trivial for existing VLMs which operate on 2D images \u2013\nas quoted from MOKA [97], \u201ccurrent VLMs are not capable of reliably predicting 6-DoF motions\u201d\nand PIVOT [98], \u201cgeneralizing to higher dimensional spaces such as rotation poses even additional\nchallenges\u201d. To tackle this, one key insight from ReKep is that VLMs only need to implicitly specify\nfull 3D rotations by reasoning about keypoints in (x, y, z) Cartesian coordinates. After this, actual\n3D rotations are solved by high-precision and efficient numerical solvers, effectively sidestepping\nthe challenge of explicitly predicting 3D rotations. As a result, the same formulation also naturally\ngeneralizes to controlling multiple arms.\nHigh-Level Planning: While many works also consider multi-stage tasks via an language-based\ntask planners which are independent from their methods, our formulation takes inspiration from\nTAMP and organically integrates high-level task planning with low-level actions in a unified contin-\nuous mathematical program. As a result, the method can naturally consider geometric dependencies\nacross stages and do so at a real-time frequency . When a failure occurs, it would backtrack to a\nprevious stage in which its conditions can still be satisfied. For example, in the \u201cpouring tea\u201d task,\nthe robot can only start tilting the teapot when the teapot spout is aligned with the cup opening.\nHowever, if the cup is being moved in the process, it should level the teapot and re-align with the\ncup. Or if the teapot is being taken from the gripper, it should instead re-grasp the teapot.\nLow-Level Execution: A common issue with using VLMs is that it is computationally expensive\nto run, hindering the high-frequency perception-action feedback loops often required for many ma-\nnipulation tasks. As a result, most of existing works either consider the open-loop settings where\nvisual perception is only used in the beginning or only consider the tasks where slow execution is\nacceptable. Instead, our formulation natively supports a high-frequency perception-action loops by\ncoupling VLMs with a point tracker, which effectively enables reactive behaviors via closed-loop\nexecution despite leveraging very large foundation models.\nVisual Prompting Methods: We uniquely consider using visual prompting for code-generation,\nwhere code may contain arbitrary arithmetic operations on a set of keypoints via visual referring\nexpressions. Although a single point is limiting to capture complex geometric structure, multiple\npoints and their relations can even specify vectors, surfaces, volumes, and their temporal dependen-\ncies. While being conceptually simple, this offers a much higher degree of flexibility which can\nfully specify 6 DoF or even 12 DoF motions.\n27",
      "content_preview": "Path Length: We approximate the path length using the dense samples by taking the sum of their\ndiffe...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 26,
        "page_label": "27",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 67.53613103324973
    },
    {
      "content": "2 Related Works\nStructural Representations for Manipulation.Structural representations determine the orchestra-\ntion of different modules in a manipulation system and yield different implications on the capabili-\nties, assumptions, efficiency, and effectiveness of the system. Rigid-body poses are most commonly\nused given well-understood rigid-body motions in free space and their efficiency at modeling long-\nrange dependencies of objects [1, 8\u201318]. However, since it often requires both geometry and dynam-\nics of environment to be modeled beforehand, various works have studied structural representations\nusing data-driven methods, such as learning object-centric representation [19\u201334], particle-based\ndynamics [35\u201341], and keypoints or descriptors [3, 4, 42\u201354]. Among them, keypoints have shown\ngreat promises given their interpretability, efficiency, generalization to instance variations [4], and\nability to model both rigid bodies and deformable objects. However, manual annotation is required\nper task, thus lacking scalability in open-world settings, which we aim to address in this work.\nConstrained Optimization in Manipulation. Constraints are often used to impose desired be-\nhaviors on robots. Motion planning algorithms use geometric constraints to compute feasible tra-\njectories that avoid obstacles and achieve goals [55\u201360]. Contact constraints can be used to plan\nforceful or contact-rich behaviors [61\u201371]. For sequential manipulation tasks, task and motion plan-\nning (TAMP) [1, 8, 13] is a widely used framework, often formulated as constraint satisfaction\nproblems [11, 72\u201377] with continuous geometric problems as subroutines. Logic-Geometric Pro-\ngramming [78\u201383] alternatively formulates a nonlinear constrained program over the entire state\ntrajectory, taking into account both logical and geometric constraints. Constraints can either be\nmanually written or learned from data in the form of manifolds [84], feasibility model [83, 85], or\nsigned-distance fields [2, 86]. Inspired by [7], we formulate sequential manipulation tasks as an\nintegrated continuous mathematical program that is repeatedly solved in a receding-horizon fashion,\nwith the key difference being that the constraints are synthesized by foundation models.\nFoundation Models for Robotics.Leveraging foundation models for robotics is an active area of\nresearch. We refer readers to [87\u201390] for overview and recent applications. Here, we focus on VLMs\nthat are capable of incorporating visual inputs [6, 91\u201395] for robotic manipulation. However, de-\nspite showing promises for open-world planning and goal specification [96\u2013110], the caption-guided\npertaining scheme of VLMs often limits visual details that can be retained about the image [111\u2013\n114]. Self-supervised vision models (e.g., DINO [5, 115]), on the other hand, provide fine-grained\npixel-level features useful for various vision and robotic tasks [31, 116\u2013121], but there lack effective\nways for interpreting open-world semantics, which is pivotal for cross-task generalization. In this\nwork, we leverage their complementary strengths by using DINOv2 [5] for fine-grained keypoint\nproposal and using GPT-4o [6] for its visual reasoning capability in a supported output modality\n(code). Similar forms of visual prompting techniques are also explored in concurrent works [96\u2013\n98, 109, 122]. In this work, we demonstrate ReKep possesses the unique advantages of performing\nchallenging 6-12 DoF tasks, integrated high-level reasoning for reactive replanning, high-frequency\nclosed-loop execution, and generating black-box constraints via visual prompting. We defer more\ndetailed discussion to Appendix A.10.\n3 Method\nHerein we discuss: (1) What are Relational Keypoint Constraints (Sec. 3.1)? (2) How to formulate\nmanipulation as a constrained optimization problem with ReKep (Sec. 3.2)? (3) What is our algo-\nrithmic instantiation that can efficiently solve the optimization in real-time (Sec. 3.3)? (4) How to\nautomatically obtain ReKep from RGB-D observations and language instructions (Sec. 3.4)?\n3.1 Relational Keypoint Constraints (ReKep)\nHerein we define a single instance of ReKep. For clarity, we assume that a set of K keypoints have\nbeen specified (discussed later in Sec. 3.4). Concretely, each keypoint ki \u2208 R3 refers to a 3D point\n3",
      "content_preview": "2 Related Works\nStructural Representations for Manipulation.Structural representations determine the...",
      "document_name": "rekep.pdf",
      "metadata": {
        "author": "Anonymous Submission",
        "creationdate": "2024-09-03T06:36:35+00:00",
        "creator": "LaTeX with hyperref",
        "keywords": "",
        "moddate": "2024-09-03T06:36:35+00:00",
        "page": 2,
        "page_label": "3",
        "producer": "pdfTeX-1.40.25",
        "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
        "subject": "Proceedings of the 8th Conference on Robot Learning (CoRL 2024)",
        "title": "",
        "total_pages": 31,
        "trapped": "/False"
      },
      "relevance_score": 67.34921452271767
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:44
CURL COMMAND:
curl -X POST http://localhost:8080/query -H "Content-Type: application/json" -d '{"query": "What are the key points in this document?"}'

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "answer": "**Key Points in the Document:**\n\n1. **Project Focus**: The internship is centered on integrating machine vision algorithms into real-world robotic systems for industrial applications, particularly for robotics.\n\n2. **Thesis Work**: This includes object detection using Dino-X and 3D reconstruction with Gaussian Splatting, which are essential for improving robotic manipulation capabilities.\n\n3. **Self-Driving Car Background**: Experience from a team project involving robotics and AI in self-driving environments, showcasing skills in hardware integration and real-world application of theoretical knowledge.\n\nThese points summarize the document's main themes and contributions to the internship participant's goals.",
  "query": {
    "kb_id": null,
    "original": "What are the key points in this document?"
  },
  "sources": [
    {
      "content": "Dear Hiring Manager,\n\nI am writing to express my interest in the Summer internship at SICK Sensor Intelligence (Analyze Edge Learning AI Vision Performance with Incomplete Data). My career goal is to integrate algorithms with hardware, bridging the gap between mathematical models and real-world engineering applications, particularly in robotics.\n\nI am currently pursuing a Master\u2019s in Applied Mathematics at KTH, where my focus has always been on applying mathematical algorithms to engineering challenges. For qualification, I had courses GPU programming, Computer Vision and Applied Machine Learning during Master's study at KTH. However, I understand that coursework alone is not enough for developing complex products like those at SICK, which is why I actively seek hands-on experience in engineering applications.\n\nThe Master Thesis I'm working on now is about Object Understanding (Dino-x) and 3D Reconstruction (Gaussian Splatting) to inform UR5 robotic arm to manipulate. Additionally, I did a interesting embedding project (self-driving car) during my Bachelor, which was a exciting team work (from unknown racing car team to high rank nationally). Beyond academia, I'm also self-motivated with frontier of artificial intelligence, my colleagues and I are currently developing an application that creates a local knowledge base for Large Language Models by tokenizing documents into vector representations.\n\nThis internship aligns perfectly with my interests, as it offers an opportunity to deepen my understanding of machine vision techniques used in industry. I believe that 3D reconstruction will play a crucial role in industry environment recognition, as 2D information alone may overlook potential risks in factory settings.\n\nReally looking forward to an interview to discuss more on computer vision & 3D Reconstruction and what I can contribute to your team.\n\nSincerely, Jiacheng Xu",
      "content_preview": "Dear Hiring Manager,\n\nI am writing to express my interest in the Summer internship at SICK Sensor In...",
      "document_name": "1740686507.860899_coverletter_SICK.pdf",
      "metadata": {},
      "relevance_score": 74.30646126838448
    },
    {
      "content": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng Xu & Dr. Sichao Liu\n\nBackground\n\nRobotic manipulation in unstructured environments remains a significant challenge in robotics research. This project aims to integrate techniques Dino-X [2] for open-vocabulary object detection, Gaussian Splatting [3] for 3D scene reconstruction, and ReKep [1] for task decomposition and spatial-temporal reasoning\u2014to enable robots to perform complex manipulation tasks. Specifically, the system will address tasks such as identifying and grasping objects like a cup, even in cluttered or dynamic environments. ReKep provides a novel approach to representing manipulation constraints using Relational Keypoint Con- straints (RKC), which are optimized through a hierarchical procedure to compute robot actions.\n\nApplication\n\nThe proposed framework targets applications in robotic manipulation for tasks such as pick-and-place, grasping, and task generalization in real-world environments like homes and warehouses. Using visual and language inputs (e.g., \u201dpick up a cup\u201d), the system will identify objects in 2D scenes, reconstruct their 3D representations, and compute optimal actions to achieve the task, even in scenarios involving task failure or long-horizon planning.\n\nProblems\n\nThe project involves solving several key mathematical and computational challenges:\n\nOpen-Vocabulary Object Detection and Segmentation: Efficiently identify- ing objects and generating precise masks using DinoX.\n\n3D Scene Reconstruction: Constructing rough but accurate object representa- tions using Gaussian Splatting for spatial understanding.\n\nTask Decomposition and Optimization: Formulating and solving tasks as se- quences of Relational Keypoint Constraints (RKC) using numerical optimization techniques.\n\nGeneralization and Error Handling: Developing algorithms for task failure detection and generalization across diverse tasks and environments.\n\n1\n\nTime Schedule\n\nMonth 1-2: Literature review and setup of development environment.\n\nMonth 3-4: Integration of DinoX for object detection and Gaussian Splatting for 3D reconstruction.\n\nMonth 5: Development of ReKep-based task decomposition and optimization framework.\n\nMonth 6: Final analysis, report writing, and thesis submission.\n\nReferences\n\n[1] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024.\n\n[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139\u20131, 2023.\n\n[3] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: A unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024.\n\n2",
      "content_preview": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng X...",
      "document_name": "1740686220.989888_Proposal.pdf",
      "metadata": {},
      "relevance_score": 69.8640179938826
    },
    {
      "content": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng Xu & Dr. Sichao Liu\n\nBackground\n\nRobotic manipulation in unstructured environments remains a significant challenge in robotics research. This project aims to integrate techniques Dino-X [2] for open-vocabulary object detection, Gaussian Splatting [3] for 3D scene reconstruction, and ReKep [1] for task decomposition and spatial-temporal reasoning\u2014to enable robots to perform complex manipulation tasks. Specifically, the system will address tasks such as identifying and grasping objects like a cup, even in cluttered or dynamic environments. ReKep provides a novel approach to representing manipulation constraints using Relational Keypoint Con- straints (RKC), which are optimized through a hierarchical procedure to compute robot actions.\n\nApplication\n\nThe proposed framework targets applications in robotic manipulation for tasks such as pick-and-place, grasping, and task generalization in real-world environments like homes and warehouses. Using visual and language inputs (e.g., \u201dpick up a cup\u201d), the system will identify objects in 2D scenes, reconstruct their 3D representations, and compute optimal actions to achieve the task, even in scenarios involving task failure or long-horizon planning.\n\nProblems\n\nThe project involves solving several key mathematical and computational challenges:\n\nOpen-Vocabulary Object Detection and Segmentation: Efficiently identify- ing objects and generating precise masks using DinoX.\n\n3D Scene Reconstruction: Constructing rough but accurate object representa- tions using Gaussian Splatting for spatial understanding.\n\nTask Decomposition and Optimization: Formulating and solving tasks as se- quences of Relational Keypoint Constraints (RKC) using numerical optimization techniques.\n\nGeneralization and Error Handling: Developing algorithms for task failure detection and generalization across diverse tasks and environments.\n\n1\n\nTime Schedule\n\nMonth 1-2: Literature review and setup of development environment.\n\nMonth 3-4: Integration of DinoX for object detection and Gaussian Splatting for 3D reconstruction.\n\nMonth 5: Development of ReKep-based task decomposition and optimization framework.\n\nMonth 6: Final analysis, report writing, and thesis submission.\n\nReferences\n\n[1] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024.\n\n[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139\u20131, 2023.\n\n[3] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: A unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024.\n\n2",
      "content_preview": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng X...",
      "document_name": "1740677023.398497_Proposal.pdf",
      "metadata": {},
      "relevance_score": 69.8640179938826
    },
    {
      "content": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng Xu & Dr. Sichao Liu\n\nBackground\n\nRobotic manipulation in unstructured environments remains a significant challenge in robotics research. This project aims to integrate techniques Dino-X [2] for open-vocabulary object detection, Gaussian Splatting [3] for 3D scene reconstruction, and ReKep [1] for task decomposition and spatial-temporal reasoning\u2014to enable robots to perform complex manipulation tasks. Specifically, the system will address tasks such as identifying and grasping objects like a cup, even in cluttered or dynamic environments. ReKep provides a novel approach to representing manipulation constraints using Relational Keypoint Con- straints (RKC), which are optimized through a hierarchical procedure to compute robot actions.\n\nApplication\n\nThe proposed framework targets applications in robotic manipulation for tasks such as pick-and-place, grasping, and task generalization in real-world environments like homes and warehouses. Using visual and language inputs (e.g., \u201dpick up a cup\u201d), the system will identify objects in 2D scenes, reconstruct their 3D representations, and compute optimal actions to achieve the task, even in scenarios involving task failure or long-horizon planning.\n\nProblems\n\nThe project involves solving several key mathematical and computational challenges:\n\nOpen-Vocabulary Object Detection and Segmentation: Efficiently identify- ing objects and generating precise masks using DinoX.\n\n3D Scene Reconstruction: Constructing rough but accurate object representa- tions using Gaussian Splatting for spatial understanding.\n\nTask Decomposition and Optimization: Formulating and solving tasks as se- quences of Relational Keypoint Constraints (RKC) using numerical optimization techniques.\n\nGeneralization and Error Handling: Developing algorithms for task failure detection and generalization across diverse tasks and environments.\n\n1\n\nTime Schedule\n\nMonth 1-2: Literature review and setup of development environment.\n\nMonth 3-4: Integration of DinoX for object detection and Gaussian Splatting for 3D reconstruction.\n\nMonth 5: Development of ReKep-based task decomposition and optimization framework.\n\nMonth 6: Final analysis, report writing, and thesis submission.\n\nReferences\n\n[1] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024.\n\n[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139\u20131, 2023.\n\n[3] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: A unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024.\n\n2",
      "content_preview": "Master Thesis Proposal: Integration of New Computer Vision Tech for Robotic Manipulation\n\nJiacheng X...",
      "document_name": "1740601690.601194_Proposal.pdf",
      "metadata": {},
      "relevance_score": 69.8640179938826
    }
  ]
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:44
CURL COMMAND:
curl -X DELETE http://localhost:8080/conversations/16

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "message": "conversation deleted"
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:44
CURL COMMAND:
curl -X DELETE http://localhost:8080/documents/46

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "message": "document deleted"
}


================================================================================
TIMESTAMP: 2025-03-26 21:21:44
CURL COMMAND:
curl -X DELETE http://localhost:8080/knowledge-bases/59

RESPONSE STATUS: 200
RESPONSE BODY:
{
  "message": "knowledge base deleted"
}
